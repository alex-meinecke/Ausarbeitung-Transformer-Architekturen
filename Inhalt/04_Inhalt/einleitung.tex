\chapter{Einleitung}

Transformer-Architekturen haben die Landschaft der neuronalen Netze revolutioniert und neue Maßstäbe in Bereichen wie maschineller Übersetzung, Textgenerierung und Sprachverarbeitung gesetzt.  

\section{Motivation und Ziel der Arbeit}

Der Grund, warum Transformer im Bereich der Textanalyse und -verarbeitung so gut sind, ist ihre einzigartige Architektur.  
Sie sind dadurch viel effizienter, schneller und erzielen auch bessere Ergebnisse.  
Die klassische Transformer-Architektur wurde 2017 in einem Paper mit dem Titel \enquote{Attention is All You Need} vorgestellt.  
Diese Grundlagen zu verstehen, ermöglicht es auch, bekannte Modelle wie BERT, GPT oder T5 zu verstehen.  
Ziel dieser Arbeit ist es, dem Leser ohne viel Vorwissen die klassischen Konzepte der Transformer-Architektur zu verdeutlichen.  

\section{Aufbau der Arbeit}

Zunächst werden im folgenden Kapitel dem Leser Grundlagen von neuronalen Netzen vermittelt, die helfen, die Notwendigkeit von Transformern in der Geschichte von neuronalen Netzen zu verstehen.  
Anschließend wird sich ein Kapitel mit dem Kernkonzept von Transformern beschäftigen, das dem Model hilft, den Kontext von Texten schnell und effizient zu analysieren.  
Dies wird zusätzlich mit einem praktischen Beispiel erläutert.  
Darauf folgt die Erklärung der Transformer-Architektur und ihrer Komponenten im Detail.  
Am Ende folgt noch ein Fazit, das noch einmal die wichtigsten Aspekte zusammenfasst.  
