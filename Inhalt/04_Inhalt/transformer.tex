\chapter{Transformer-Architektur im Detail}

Im ursprünglichen Werk \textit{Attention Is All You Need} \todo{cite}, in dem das grundlegende Transformer-Modell vorgestellt wurde, besteht ein Transformer aus zwei Hauptkomponenten: dem Encoder und dem Decoder.

Der \textbf{Encoder} übersetzt die Eingabewörter in eine abstrahierte Repräsentation.  
Ziel ist es hierbei, die semantischen und syntaktischen Informationen des Textes zu extrahieren und zu kodieren.  
Die andere Komponente, der \textbf{Decoder}, nutzt diese Repräsentation, um passende Outputs zu generieren. Dabei berechnet er in jeder Iteration den nächstbesten Token, der zum Output hinzugefügt werden soll. Hierbei bezieht er auch den zuletzt generierten Token in die Berechnung des nächsten Tokens ein.

\section{Komponenten in einem Transformer}

Um die Architektur eines Transformers zu verstehen, müssen zunächst die Komponenten betrachtet werden, die überall in der Architektur eingesetzt werden und sich gleich verhalten.

\subsection{Multi-Head-Attention}

Wie schon im vorherigen Kapitel beschrieben, ist Self-Attention ein zentraler Baustein in der Transformer-Architektur.  
Um Transformer noch effizienter zu gestalten, wird Self-Attention in Form der \textbf{Multi-Head-Attention} angewendet. Dabei beschreibt jeder einzelne Kopf einen eigenen Attention-Zyklus, wie oben beschrieben.

Jeder Kopf verwendet unterschiedliche Gewichtungsmatrizen, um den Fokus bei der Analyse der Zusammenhänge zwischen Tokens auf verschiedene Schwerpunkte zu setzen.  
So kann beispielsweise ein Kopf die Tokens hinsichtlich syntaktischer Beziehungen analysieren, während ein anderer die semantischen Beziehungen betrachtet.  
Für jeden Attention-Kopf \( h \) werden dabei unterschiedliche Gewichtungsmatrizen \( \mathbf{W}_{Qh} \), \( \mathbf{W}_{Kh} \) und \( \mathbf{W}_{Vh} \) verwendet.

Viele Transformermodelle verwenden acht Attention-Köpfe, die parallel ausgeführt werden.  
Nach der parallelen Verarbeitung werden die jeweiligen Ergebnisse der Köpfe zusammengefügt (auch als Konkatenieren bezeichnet) und mit Hilfe einer weiteren Gewichtungsmatrix \( \mathbf{W}_{O} \) zusammengefasst.  
\( \mathbf{W}_{O} \) ist die Matrix, welche die Ergebnisse der Attention-Köpfe unterschiedlich gewichtet.  
Umgangssprachlich formuliert bildet sie eine einheitliche Schlussfolgerung für den Transformer aus dem Multi-Head-Attention-Zyklus.

\subsection{Positional Encodings}

Ein Problem bei der einfachen Verwendung von Self-Attention ist, dass die tatsächliche Reihenfolge der Tokens verloren geht.  
Der Grund dafür ist, dass die Eingabe für den Algorithmus als eine Menge betrachtet wird, und in einer Menge haben Elemente keine feste Reihenfolge.  
Das Ergebnis ist somit unabhängig von der Reihenfolge der Tokens, beispielsweise in einem Satz.

Hier kommt das \enquote{Positional Encoding} ins Spiel.  
Dabei werden zu den ursprünglichen Embeddings, die aus den Tokens generiert wurden, Positionsdaten hinzugefügt, die der Transformer erkennen kann.  
Hierfür verwendet man abwechselnd Sinus- und Cosinus-Funktionen:

\[
PE_{(pos, 2i)} = \sin\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right), \quad
PE_{(pos, 2i+1)} = \cos\left(\frac{\text{pos}}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
\]

Hierbei ist \( \text{pos} \) die Position des Tokens, \( i \) ist die aktuell zu betrachtende Dimension für diesen Token, und \( d_{\text{model}} \) ist die Anzahl der Dimensionen im Modell.

Die unterschiedlichen Frequenzen der Sinus- und Cosinus-Funktionen ermöglichen es, dass jede Dimension des Embeddings anders auf die Position reagiert.  
Sinus und Cosinus haben eine periodische Struktur, die es dem Transformer erlaubt, die Position der Tokens in der ursprünglichen Eingabe sowie die Positionsunterschiede zwischen Tokens zu ermitteln.  
Die abwechselnde Verwendung dieser Funktionen erleichtert die Berechnung der relativen Unterschiede zwischen zwei Positionen.  
Sie sind orthogonal zueinander, was bedeutet, dass sie sich nie überlagern und somit unabhängige Informationen darstellen.
