\chapter{Transformer-Architektur im Detail}

Im ursprünglichen Werk \textit{Attention Is All You Need} \todo{cite}, in dem das grundlegende Transformer-Modell vorgestellt wurde, besteht ein Transformer aus zwei Hauptkomponenten: 
dem Encoder und dem Decoder.

Encoder und Decoder verarbeiten Text in Form von Tokens, die ganze Wörter oder Wortteile sein können. 
Jeder Token ist für den Transformer einzigartig und wird zunächst nur durch eine natürliche Zahl repräsentiert.

Der \textbf{Encoder} übersetzt die Eingabewörter in eine abstrahierte Repräsentation.  
Ziel ist es hierbei, die semantischen und syntaktischen Informationen des Textes zu extrahieren und zu kodieren.  
Die andere Komponente, der \textbf{Decoder}, nutzt diese Repräsentation, um passende Outputs zu generieren. 
Dabei berechnet er in jeder Iteration den nächsten am besten passenden Token, der zum Output hinzugefügt werden soll. 
Hierbei bezieht er auch den zuletzt generierten Token in die Berechnung des nächsten Tokens ein.

Grundlage für sowohl den Encoder als auch den Decoder ist das Self-Attention-Konzept, das im folgenden Abschnitt näher erläutert wird.

