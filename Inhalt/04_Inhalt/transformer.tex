\chapter{Transformer-Architektur im Detail}

Im ursprünglichen Werk \textit{Attention Is All You Need} \todo{cite}, in dem das grundlegende Transformer-Modell vorgestellt wurde, besteht ein Transformer aus zwei Hauptkomponenten: 
dem Encoder und dem Decoder.

Encoder und Decoder verarbeiten Text in Form von Tokens, die ganze Wörter oder Wortteile sein können. 
Jeder Token ist für den Transformer einzigartig und wird zunächst nur durch eine natürliche Zahl repräsentiert.

Der \textbf{Encoder} übersetzt die Eingabewörter in eine abstrahierte Repräsentation.  
Ziel ist es hierbei, die semantischen und syntaktischen Informationen des Textes zu extrahieren und zu kodieren.  
Die andere Komponente, der \textbf{Decoder}, nutzt diese Repräsentation, um passende Outputs zu generieren. 
Dabei berechnet er in jeder Iteration den nächsten am besten passenden Token, der zum Output hinzugefügt werden soll. 
Hierbei bezieht er auch den zuletzt generierten Token in die Berechnung des nächsten Tokens ein.

Grundlage für sowohl den Encoder als auch den Decoder ist das Self-Attention-Konzept, das im folgenden Abschnitt näher erläutert wird.

\section{Wie funktioniert Attention?}

Ein Attention-Zyklus kann als Übersetzungsfunktion von Eingabevektoren zu Ausgabevektoren verstanden werden.

\subsection{Generierung von Embeddings}

Grundlage jedes Attention-Zyklus sind Eingabe-Tokens.  
Um die mathematische Vorgehensweise besser zu veranschaulichen, wird im Folgenden der Satz \enquote{Ich sitze auf der Bank} als Beispiel verarbeitet.
Jedes dieser Wörter ist ein eigener Token, der vor der Eingabe in den Attention-Zyklus vom Transformer übersetzt wurde:
[\enquote{Ich}, \enquote{sitze}, \dots, \enquote{Bank}].  
Diese Tokens könnten für das Modell wie folgt aussehen: [\enquote{243}, \enquote{645}, \dots, \enquote{316}].
Die Herausforderung für den Transformer besteht darin, aus dem Kontext der anderen Tokens zu erkennen, ob \enquote{Bank} eine Sitzbank oder das Finanzinstitut Bank bedeutet.

Jeder Token bildet ein Schlüssel-Werte-Paar.  
Der korrespondierende Wert hinter einem Token ist ein Vektor, der die Bedeutung eines Tokens hinsichtlich mehrerer Dimensionen beschreibt.  
Diese Vektoren sind aus Trainingsdaten des Transformer-Modells entstanden.

Im ersten Schritt des Attention-Zyklus wird jeder Token in den dazugehörigen Vektor übersetzt.  
Diese Vektoren werden in der Matrix $\mathbf{X}$ gespeichert, wobei jede Zeile einen Token repräsentiert.  
Dieser Prozess wird als \textbf{Embedding} bezeichnet.  
Jeder dieser Vektoren hat gemäß der Literatur mindestens 512 Dimensionen.  
Es wird von einem \( d_{\text{model}} = 512 \) gesprochen.  
Es gilt: Je größer das \( d_{\text{model}} \), desto präziser kann der Transformer die Zusammenhänge zwischen Tokens erkennen.

Wenn sich Tokens im Vektorraum nahe liegen, haben sie eher Gemeinsamkeiten im Vergleich zu Tokens, die weit auseinander liegen.  
Angenommen, es gäbe nur ein \( d_{\text{model}} = 2 \) für jeden Token, könnten diese zwei Dimensionen als Koordinaten genutzt werden, um Zusammenhänge visuell als Cluster in einem Koordinatensystem darzustellen.  
Hier wären beispielsweise die Tokens \enquote{Hund} und \enquote{Katze} nah beieinander.

Für das oben genannte Beispiel \enquote{Ich sitze auf der Bank} nehmen wir der Übersichtlichkeit halber ein \( d_{\text{model}} = 4 \).  
So ergibt sich eine Embedding-Matrix $\mathbf{X}$ mit 5 Zeilen für 5 Tokens und 4 Spalten für jeweils 4 Dimensionen:

\[
\centering
\mathbf{X} =
\begin{bmatrix}
0.4 & 0.8 & 1.5 & 1.6 \\
3.2 & 0.4 & 0.7 & 0.2 \\
0.6 & 0.9 & 1.2 & 0.5 \\
2.1 & 0.5 & 2.0 & 0.2 \\
0.7 & 2.4 & 0.1 & 0.9
\end{bmatrix}
\]

\subsection{Lineare Transformation in Query-, Key- und Value-Matrizen}

Die Embedding-Matrix $\mathbf{X}$ wird durch drei Gewichtungsmatrizen $\mathbf{W_Q}$, $\mathbf{W_K}$ und $\mathbf{W_V}$, die aus dem Training des Transformer-Modells stammen, in drei neue Matrizen transformiert:

\[
\mathbf{Q} = \mathbf{X} \cdot \mathbf{W_Q}
\]
\[
\mathbf{K} = \mathbf{X} \cdot \mathbf{W_K}
\]
\[
\mathbf{V} = \mathbf{X} \cdot \mathbf{W_V}
\]

Die drei Matrizen haben im Attention-Zyklus umgangssprachlich formuliert folgende Funktionen:

\begin{itemize}
    \item \textbf{Query-Matrix (\(\mathbf{Q}\))}: Was fragt ein Token?
    \item \textbf{Key-Matrix (\(\mathbf{K}\))}: Welche Tokens im Kontext antworten am besten auf die Frage?
    \item \textbf{Value-Matrix (\(\mathbf{V}\))}: Welche Informationen fließen letztendlich in den nächsten Schritt ein?
\end{itemize}

Im Beispiel könnten die jeweiligen Zeilen der Matrizen \(\mathbf{Q}\), \(\mathbf{K}\), \(\mathbf{V}\) für das Token \enquote{Bank} folgendermaßen aussehen:

\[
\begin{aligned}
\math{Q}_{\text{Bank}} &= [1.0, 0.7, 0.9, 1.1], \quad 
\math{K}_{\text{Bank}} &= [0.8, 0.6, 1.0, 0.9], \quad 
\math{V}_{\text{Bank}} &= [0.9, 0.5, 0.7, 1.0]
\end{aligned}
\]

\subsection{Berechnung der Attention-Scores}
